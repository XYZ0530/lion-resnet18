{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Function to load metrics from a JSON file\n",
    "def load_metrics_from_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Directory where your JSON files are saved\n",
    "adam_directory = './adam_metrics/'\n",
    "lion_directory = './lion_metrics/'\n",
    "\n",
    "# Lists to hold metrics from all runs\n",
    "adam_train_time = []\n",
    "adam_accuracy = []\n",
    "adam_precision = []\n",
    "adam_recall = []\n",
    "\n",
    "lion_train_time = []\n",
    "lion_accuracy = []\n",
    "lion_precision = []\n",
    "lion_recall = []\n",
    "\n",
    "# Load metrics from each JSON file for Adam optimizer\n",
    "for filename in os.listdir(adam_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        metrics = load_metrics_from_json(os.path.join(adam_directory, filename))\n",
    "        adam_train_time.append(sum(metrics['train_time']) / len(metrics['train_time']))\n",
    "        adam_accuracy.append(metrics['test_accuracy'])\n",
    "        adam_precision.append(metrics['test_precision'])\n",
    "        adam_recall.append(metrics['test_recall'])\n",
    "\n",
    "# Load metrics from each JSON file for Lion optimizer\n",
    "for filename in os.listdir(lion_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        metrics = load_metrics_from_json(os.path.join(lion_directory, filename))\n",
    "        lion_train_time.append(sum(metrics['train_time']) / len(metrics['train_time']))\n",
    "        lion_accuracy.append(metrics['test_accuracy'])\n",
    "        lion_precision.append(metrics['test_precision'])\n",
    "        lion_recall.append(metrics['test_recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average of a list\n",
    "def calculate_average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "# Calculate averages for Adam optimizer\n",
    "average_adam_train_time = calculate_average(adam_train_time)\n",
    "average_adam_accuracy = calculate_average(adam_accuracy)\n",
    "average_adam_precision = calculate_average(adam_precision)\n",
    "average_adam_recall = calculate_average(adam_recall)\n",
    "\n",
    "# Calculate averages for Lion optimizer\n",
    "average_lion_train_time = calculate_average(lion_train_time)\n",
    "average_lion_accuracy = calculate_average(lion_accuracy)\n",
    "average_lion_precision = calculate_average(lion_precision)\n",
    "average_lion_recall = calculate_average(lion_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Time (Adam): 403.37 seconds\n",
      "Average Training Time (Lion): 270.62 seconds\n",
      "Average Reduction in Training Time: 132.76 seconds (32.91% faster)\n",
      "\n",
      "Average Accuracy (Adam): 78.4000%\n",
      "Average Accuracy (Lion): 81.1620%\n",
      "Average Improvement in Accuracy: 2.7620%\n",
      "\n",
      "Average Precision (Adam): 0.8082\n",
      "Average Precision (Lion): 0.8159\n",
      "Average Improvement in Precision: 0.0077\n",
      "\n",
      "Average Recall (Adam): 0.7840\n",
      "Average Recall (Lion): 0.8116\n",
      "Average Improvement in Recall: 0.0276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate improvements of Lion over Adam\n",
    "improvement_accuracy = average_lion_accuracy - average_adam_accuracy\n",
    "improvement_precision = average_lion_precision - average_adam_precision\n",
    "improvement_recall = average_lion_recall - average_adam_recall\n",
    "improvement_train_time = average_adam_train_time - average_lion_train_time  \n",
    "improvement_train_time_percent = (improvement_train_time/average_adam_train_time) * 100 \n",
    "\n",
    "# Print results\n",
    "print(f\"Average Training Time (Adam): {average_adam_train_time:.2f} seconds\")\n",
    "print(f\"Average Training Time (Lion): {average_lion_train_time:.2f} seconds\")\n",
    "print(f\"Average Reduction in Training Time: {improvement_train_time:.2f} seconds ({improvement_train_time_percent:.2f}% faster)\\n\")\n",
    "\n",
    "print(f\"Average Accuracy (Adam): {average_adam_accuracy:.4f}%\")\n",
    "print(f\"Average Accuracy (Lion): {average_lion_accuracy:.4f}%\")\n",
    "print(f\"Average Improvement in Accuracy: {improvement_accuracy:.4f}%\\n\")\n",
    "\n",
    "print(f\"Average Precision (Adam): {average_adam_precision:.4f}\")\n",
    "print(f\"Average Precision (Lion): {average_lion_precision:.4f}\")\n",
    "print(f\"Average Improvement in Precision: {improvement_precision:.4f}\\n\")\n",
    "\n",
    "print(f\"Average Recall (Adam): {average_adam_recall:.4f}\")\n",
    "print(f\"Average Recall (Lion): {average_lion_recall:.4f}\")\n",
    "print(f\"Average Improvement in Recall: {improvement_recall:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
